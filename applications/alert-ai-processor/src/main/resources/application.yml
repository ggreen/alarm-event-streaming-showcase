server:
  port: 0
spring:
  application:
    name: alert-ai-processor
  ai:
    model:
      embedding: ollama
      chat: ollama
    ollama:
      base-url: http://localhost:11434
      chat:
        options:
          format: "json"
          model: llama3.1:8b
  cloud:
    function:
      definition: alertAiProcessor;checkForAlerts
    stream:
      default:
        contentType: "application/json"
      bindings:
        alertAiProcessor-in-0:
          destination: activities.activity
          group: activities.activity
          consumer:
            concurrency: 1
        alertAiProcessor-out-0:
          destination: alerts.alert
          group: alerts.alert
        checkForAlerts-out-0:
          destination: alerts.alert
          group: alerts.alert
      rabbit:
        bindings:
          alertAiProcessor-in-0:
            consumer:
              containerType: STREAM
              queueNameGroupOnly: true
          alertAiProcessor-out-0:
            producer:
              producerType: STREAM_SYNC
              queueNameGroupOnly: true
          checkForAlerts-out-0:
            producer:
              producerType: STREAM_SYNC
              queueNameGroupOnly: true
#-----------------------
alerts:
  inference:
    batch: 5
  header:
    names:
      account: "account"
      level: "level"